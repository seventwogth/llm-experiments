# Заметки по проекту GAN (Генерация цветовых палитр)

## 1. Блок генерации цвета (HSV to RGB)

```python
def hsv_to_rgb(h, s, v):
    return colorsys.hsv_to_rgb(h, s, v)

def generate_harmonious_palette(n_colors=5):
    h = np.random.rand()            # Случайный оттенок (Hue) от 0 до 1
    s = np.random.uniform(0.4, 1.0) # Насыщенность (Saturation)
    v = np.random.uniform(0.7, 1.0) # Яркость (Value)
```

**Как это работает:**
*   Мы работаем в цветовой модели **HSV** (Hue, Saturation, Value), а не RGB, потому что в HSV математически проще искать гармоничные сочетания (например, "соседние" цвета или "противоположные").
*   `hsv_to_rgb`: Это просто "обертка" над библиотечной функцией, которая переводит понятные математике HSV координаты в понятные монитору RGB координаты.
*   `generate_harmonious_palette`: Эта функция создает **базовый цвет**, от которого будет строиться вся палитра.
    *   Мы берем случайный `h` (цвет).
    *   Мы ограничиваем `s` (насыщенность) и `v` (яркость), чтобы не генерировать слишком бледные или слишком темные (почти черные) цвета. Мы хотим сочные, красивые палитры.

## 2. Цветовые схемы

```python
scheme = np.random.choice(['analogous', 'monochromatic', 'triadic'])
```

**Что это:** Это правила теории цвета, по которым выбираются остальные цвета в палитре относительно первого (базового).
**На что влияют:** На то, какие именно "реальные данные" мы скармливаем GANу. Если мы не зададим эти правила, мы будем учить сеть на случайном шуме, и она выдаст шум.

1.  **analogous (Аналоговая):** Цвета, расположенные рядом на цветовом круге. Создают спокойное, приятное сочетание.
2.  **monochromatic (Монохромная):** Один и тот же оттенок (Hue), но разная яркость и насыщенность.
3.  **triadic (Триадическая):** Три цвета, равномерно распределенные по цветовому кругу (треугольник). Это контрастные, яркие сочетания.

## 3. Цикл создания палитры

```python
for i in range(n_colors):
    # Берем базовые значения
    new_h, new_s, new_v = h, s, v
    
    if scheme == 'monochromatic':
        # Оттенок (h) не меняем! Меняем насыщенность и яркость.
        # Добавляем случайный сдвиг (jitter) к S и V.
        new_s = max(0, min(1, s + np.random.uniform(-0.3, 0.3)))
        new_v = max(0, min(1, v + np.random.uniform(-0.4, 0.1)))
        
    elif scheme == 'analogous':
        # Слегка сдвигаем оттенок (h) влево или вправо.
        # i влияет на то, насколько далеко уйдем от базы.
        new_h = (h + np.random.uniform(-0.05, 0.05) * i) % 1.0
        
    elif scheme == 'triadic':
        # Сдвигаем оттенок ровно на 1/3 круга (0.33) для каждого следующего цвета в триаде.
        if i > 0:
            new_h = (h + (1/3) * (i % 3)) % 1.0
    
    # Конвертируем полученный HSV цвет в RGB
    r, g, b = hsv_to_rgb(new_h, new_s, new_v)
    # Добавляем 3 числа (R, G, B) в список палитры
    palette.extend([r, g, b])
    
return np.array(palette, dtype=np.float32)
```

**Что происходит:**
Мы создаем вектор чисел. Если нам нужно 5 цветов, цикл пройдет 5 раз. В каждом проходе мы вычисляем `r, g, b`.
В итоге `palette` — это плоский список длиной 15 чисел: `[R1, G1, B1, R2, G2, B2, ..., R5, G5, B5]`.

## 4. Класс Dataset

```python
class PaletteDataset(Dataset):
   # ... (код выше)
```

**Что это:** Это стандартный класс PyTorch для работы с данными. Он нужен, чтобы "скормить" наши данные в `DataLoader`.
*   `__init__`: Запускается один раз при старте. Мы генерируем список из 5000 (или сколько укажем) "идеальных" палитр и сохраняем их в память (`self.data`). Это наш "Учебник" для нейросети.
*   `__len__`: Говорит PyTorch'у, сколько всего у нас примеров (5000).
*   `__getitem__`: Позволяет достать конкретную палитру по индексу. Превращает массив NumPy в Тензор PyTorch (формат, понятный видеокарте).

## 5. Sample Shape & Palette

```python
dataset[0].shape # torch.Size([15])
dataset[0]       # tensor([0.12, 0.54, 0.99, ...])
```

*   **Sample Shape (Размерность):** `[15]`. Это **не** картинка 5x3 пикселя. Для нейросети это просто строка из 15 чисел. Мы подаем ей на вход одномерный вектор.
*   **Sample Palette:** Это сам набор чисел. В RGB цвета кодируются от 0 до 1 (в PyTorch) или от 0 до 255. У нас от 0 до 1, так как `colorsys` возвращает float.

## 6. Генератор (Generator)

```python
class Generator(nn.Module):
    # ...
    self.model = nn.Sequential(
        nn.Linear(input_dim, 128),   # Слой 1: Растягиваем шум (100) до 128 признаков
        nn.ReLU(),                   # Активация: Убираем отрицательные значения
        nn.Linear(128, 256),         # Слой 2: Увеличиваем сложность до 256
        nn.ReLU(),                   # Активация
        nn.Linear(256, output_dim),  # Слой 3: Сжимаем до нужного размера (15 чисел)
        nn.Sigmoid()                 # ВАЖНО: Загоняет числа в диапазон [0, 1]
    )
```

**Зачем нужен:** Создает палитру из "ничего" (из шума).
**Слои:**
*   **Linear (Полносвязный):** Обычный слой нейросети, где каждый нейрон связан с каждым. Хорошо подходит для простых векторов данных.
*   **ReLU:** Функция активации. Помогает сети учить нелинейные зависимости.
*   **Sigmoid в конце:** Самое важное. Цвета RGB не могут быть меньше 0 или больше 1. Сигмоида принудительно превращает любое число (хоть -1000, хоть +500) в число от 0 до 1.

## 7. Дискриминатор (Discriminator)

```python
class Discriminator(nn.Module):
    # ...
    self.model = nn.Sequential(
        nn.Linear(input_dim, 256),   # Слой 1: Принимает палитру (15 чисел)
        nn.LeakyReLU(0.2),           # Активация Leaky (пропускает немного негатива)
        nn.Linear(256, 128),         # Слой 2: Сжимаем признаки
        nn.LeakyReLU(0.2),           # Активация
        nn.Linear(128, 1),           # Слой 3: Выдает ОДНО число
        nn.Sigmoid()                 # Превращает число в вероятность (0...1)
    )
```

**Зачем нужен:** Критик. Он пытается угадать, настоящая перед ним палитра (из датасета) или подделка (от генератора).
**Слои:**
*   **LeakyReLU:** В отличие от обычного ReLU, он не зануляет отрицательные числа полностью, а оставляет маленький "хвостик" (0.2). Это помогает избежать проблемы "мертвых нейронов" в GAN, когда сеть перестает учиться.
*   **Выход 1 нейрон:** Нам нужен ответ "Да" или "Нет".
*   **Sigmoid:** Превращает выход в вероятность. Ближе к 1 — "Это реал". Ближе к 0 — "Это фейк".

## 8. Sequential

**Что это:** Контейнер PyTorch (`nn.Sequential`).
**Почему он:** Он позволяет упаковать слои в "коробку" друг за другом. Данные заходят в первый слой и последовательно проходят до последнего. Это самый простой способ описать нейросеть без ветвлений. Для нашей задачи (простой перцептрон) это идеально.

## 9. Параметры обучения

```python
LR = 0.0002       # Learning Rate (Скорость обучения)
BATCH_SIZE = 64   # Размер пакета
EPOCHS = 50       # Количество проходов по всем данным
Z_DIM = 100       # Размер вектора шума
```

*   **LR = 0.0002:** "Магическое число" для GAN (пошло из статьи про DCGAN). Если сделать больше — сеть пойдет вразнос. Если меньше — будет учиться вечно.
*   **BATCH_SIZE = 64:** Мы учим сеть не по 1 примеру, а пачками по 64. Это делает обучение стабильнее и быстрее (видеокарта любит параллельность).
*   **EPOCHS = 50:** Задача простая, данные простые. 50 раз показать датасет достаточно, чтобы сеть поняла закономерности.
*   **Z_DIM = 100:** Это "семя" для генератора. Вектор из 100 случайных чисел, из которого Генератор "лепит" результат. 100 — достаточное число измерений для кодирования вариативности цветов.

## 10. Функция Train (Процесс обучения)

Это "мозг" программы.

**Основные этапы внутри цикла (`for i, real_palettes in enumerate(dataloader)`):**

1.  **Подготовка дискриминатора:**
    *   `opt_d.zero_grad()`: Обнуляем старые градиенты (чтобы не мешали).

2.  **Обучение на РЕАЛЬНЫХ данных:**
    *   Берем палитры из датасета.
    *   Создаем метки `labels_real` (единицы, так как это правда).
    *   Прогоняем через дискриминатор -> считаем ошибку (`loss_real`).

3.  **Обучение на ФЕЙКОВЫХ данных:**
    *   Генерируем шум `noise`.
    *   Генератор создает `fake_palettes`.
    *   **Важно:** Используем `.detach()` на фейковых палитрах. Мы говорим PyTorch: *"Сейчас мы учим Дискриминатор. Не вздумай менять веса Генератора, пока мы оцениваем его работу!"*.
    *   Метки `labels_fake` (нули, так как это ложь).
    *   Считаем ошибку (`loss_fake`).

4.  **Шаг Дискриминатора:**
    *   Суммируем ошибки (Real + Fake). Делаем `backward()` (вычисляем, куда крутить веса). Делаем `step()` (крутим веса).

5.  **Обучение Генератора:**
    *   `opt_g.zero_grad()`: Обнуляем градиенты генератора.
    *   Снова просим Дискриминатор оценить `fake_palettes` (теперь без detach, нам нужны градиенты сквозь дискриминатор к генератору).
    *   **Хитрость:** Мы используем метки `labels_real` (единицы)!
        *   *Смысл:* Мы "обманываем" функцию потерь. Мы говорим: *"Генератор, твоя ошибка высока, если Дискриминатор сказал '0'. Твоя ошибка низкая, если он сказал '1'"*. Мы штрафуем Генератор за то, что его раскрыли.
    *   Делаем `backward()` и `step()` для Генератора.

**Итог:** Дискриминатор учится отличать правду от лжи, а Генератор учится создавать такую ложь, которую Дискриминатор примет за правду. Это игра с нулевой суммой.
