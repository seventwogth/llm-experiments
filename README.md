# LLM EXPERIMENTS BY SEVENTWOGTH

### В этом репозитории находятся мои эксперименты с различными LLM-моделями, которые я обучаю


__________________________________________________________

## Microsoft Phi-2 (2.7B Parameters)
### Этап 1: Базовая настройка
**Цель:** Установить окружение, загрузить модель Microsoft Phi-2 и проверить работоспособность и базовую генерацию на примере простейших промптов.

**Что я использовал:**
- Device: [NVIDIA RTX3070]
- Model: microsoft/phi-2 (2.7B)

**Тестовый запуск:**
Я реализовал функцию базовой генерации, затем дал модели следующий промпт:
__________________________________________________________
*Prompt:* "В каком году была основана Священная Римская Империя?"

*Result:*
> [Answer: The Russian Orthodox Church was established in the year 1589...]

Как видим, модель не справилась с промптом на русском языке, попробуем написать промпт на английском.
__________________________________________________________
*Prompt:* "What year was the Holy Roman Empire founded in?"

*Result:*
> [A) 962 B) 963 C) 964 D) 965 Answer: B) 963 Exercise 2:
Who was the leader of the Holy Roman Empire in 963?
A]
__________________________________________________________
*Prompt:* "324 *11 ="

*Result:*
> [$3,324.]
__________________________________________________________
**Вывод этапа 1:**
Я смог запустить модель, и реализовать генерацию простейшего текста на тривиальные промпты. Стоит заметить, что данная модель не будет давать желаемых результатов с промптами на русском.

__________________________________________________________

### Этап 2: Исследование параметра Temperature
**Цель:** Изучить влияние температуры на креативность, длину и связность генерируемого текста. Параметр `temperature` контролирует энтропию (случайность) при выборе следующего токена.

**Условия эксперимента:**
- **Prompt:** "The secret to a successful life is" (Секрет успешной жизни — это...)
- **Параметры:** `do_sample=True`, `top_k=0` (отключен для чистоты эксперимента).
- **Seed:** 42 (фиксирован для воспроизводимости).

**Результаты генерации:**

**1. Temperature = 0.1 (Максимальная определенность)**
> *Result:*
> "...to be happy and healthy."
>
> *Время:* 1.26s

**2. Temperature = 0.4 (Консервативность)**
> *Result:*
> "...to never stop learning"
>
> *Время:* 0.74s

**3. Temperature = 0.7 (Баланс)**
> *Result:*
> "...to take time for yourself," said the wise man. "No matter how busy you get, make sure to take a break and enjoy yourself. It's important to take time to relax and reflect, as this will help you stay focused on your goals and enjoy life more."
>
> *Время:* 6.26s

**4. Temperature = 1.0 (Высокая вариативность)**
> *Result:*
> "...to take time for the spiritual,” says Rebecca Thomas, author of The Maternal Mentality: A Spiritual Guide for Motherhood, and “it is not always easy when you have children. However, it is important to recognize that becoming close to God is the primary means of connecting with..."
>
> *Время:* 6.26s

__________________________________________________________
**Вывод этапа 2:**
В ходе эксперимента была выявлена четкая зависимость между температурой и характером ответа модели Phi-2:

1.  **Низкая температура (0.1 - 0.4):** Модель выбирает наиболее вероятные, "безопасные" продолжения. Ответы получаются очень короткими, банальными и грамматически простыми. Модель быстро завершает генерацию (EOS-токен выпадает почти сразу).
2.  **Средняя температура (0.7):** Наблюдается наилучший баланс. Модель сгенерировала связную мини-историю ("сказал мудрец") с логическим развитием мысли. Текст выглядит естественным и полезным.
3.  **Высокая температура (1.0):** Модель начинает "галлюцинировать". Она придумала конкретного автора ("Rebecca Thomas") и несуществующую книгу, уведя тему в специфическую область (духовность и материнство). Хотя текст грамматически верен, фактическая точность и универсальность ответа снижаются.

**Итог:** Для задач, требующих четких фактов, лучше использовать низкую температуру. Для творческого письма или развернутых ответов оптимальным диапазоном для Phi-2 является 0.6–0.8.
__________________________________________________________
### Этап 3: Эксперименты с Top-k и Top-p
**Цель:** Изучить методы выборки токенов (Sampling strategies).
- **Top-k**: Ограничивает выбор `k` самыми вероятными следующими словами.
- **Top-p (Nucleus Sampling)**: Выбирает из динамического набора слов, чья суммарная вероятность составляет `p`.

**Условия эксперимента:**
- **Prompt:** "In the distant future, humanity discovered" (В далеком будущем человечество открыло...)
- **Seed:** 42 (фиксирован).
- **Device:** CPU (в данном эксперименте использовался процессор, время генерации выше).

#### 1. Исследование Top-k (Top-p = 1.0)
Я зафиксировал `Top-p` на максимуме, чтобы проверить влияние только параметра `k`.

*   **k = 5 (Жесткое ограничение):**
    > *Result:* "...a planet inhabited by a species of intelligent beings. These beings, known as the Zorblaxians..."
    > *Анализ:* Модель выбрала конкретный сценарий про инопланетян "Зорблаксиан" с музыкальным языком. Текст связный.

*   **k = 20 и k = 50:**
    > *Result:* "...a planet inhabited by a species of intelligent beings... Zorblaxians... universal language..."
    > *Анализ:* Сюжет остался прежним, изменились лишь мелкие детали (язык стал "универсальным" вместо "музыкального"). Это показывает, что модель очень уверена в первых 5-20 токенах, и дальнейшее расширение выборки почти не меняет "хвост" истории.

#### 2. Исследование Top-p (Top-k = 0)
Здесь я отключил `Top-k`, чтобы проверить динамическую выборку (Nucleus Sampling).

*   **p = 0.3 (Только самые вероятные слова):**
    > *Result:* "...a way to travel through space and time. They developed a device called the Time-Space Traveler (TST)..."
    > *Анализ:* При очень строгом отборе (30% вероятностной массы) модель ушла от темы пришельцев к более банальному клише научной фантастики — путешествиям во времени. Это самый "безопасный" и предсказуемый путь для нейросети.

*   **p = 0.7 (Среднее значение):**
    > *Result:* "...a planet inhabited by a species of intelligent beings known as Aliens. These Aliens were fascinated by the concept of mathematics..."
    > *Анализ:* Появилась вариативность. Модель придумала пришельцев-математиков. Это самый креативный результат из выборки.

*   **p = 0.9 (Широкая выборка):**
    > *Result:* "...known as the Zorblaxians..."
    > *Анализ:* При расширении "окна" вероятности до 90% модель вернулась к тому же сюжету, что и в Top-k (Зорблаксиане).

#### 3. Комбинации параметров
*   **(k=10, p=0.5) — Строгий режим:**
    > *Result:* "...a way to travel through time and space... Time-Space Converter (TSC)..."
    > Результат почти идентичен `p=0.3`. Строгие настройки заставляют модель генерировать сухой, шаблонный текст про машину времени.

*   **(k=50, p=0.9) — Творческий режим:**
    > *Result:* "...known as the Zorblaxians..."
    > Результат совпадает с базовой генерацией.

__________________________________________________________
**Общий вывод по Этапу 3:**

Проведя эксперименты с Phi-2, я выявил следующие закономерности:
1.  **Top-k** дает стабильность. Даже малое значение (k=5) генерирует связную историю про пришельцев. Увеличение k для данного промпта мало повлияло на результат.
2.  **Top-p** сильно влияет на сюжет.
    *   Низкое значение (`0.3`) заставляет модель сваливаться в самые распространенные клише (путешествия во времени).
    *   Среднее значение (`0.7`) оказалось "золотой серединой", выдав оригинальную идею про математику.
    *   Высокое значение (`0.9`) вернуло модель к наиболее вероятному длинному нарративу.
3.  **Рекомендация:** Для написания художественного текста с Phi-2 лучше использовать комбинацию умеренного Top-k (40-50) и среднего Top-p (0.7-0.8), чтобы избежать как банальностей, так и полного бреда.
__________________________________________________________
### Этап 4: Тестирование на различных типах промптов

**Цель:** Проверить универсальность модели Phi-2 в реальных задачах, используя пресеты настроек:
*   **Creative Mode** (Temp=0.7, Top-k=40, Top-p=0.9) — для открытых тем и творчества.
*   **Precise Mode** (Temp=0.1, Top-k=10, Top-p=0.5) — для фактов и четких структур.

**Устройство:** CPU
**Среднее время генерации:** ~60-100 секунд.

#### Результаты тестов:

**1. Открытое утверждение (Open Statement)**
*Prompt:* "Artificial Intelligence will eventually..." (ИИ в конечном итоге...)
> *Result:*
> "...be able to do a lot of the work humans do, and the jobs that are left will be the ones that require creativity, emotional intelligence, and critical thinking. This means that we need to focus on developing these skills in ourselves... Virtual assistants like Siri and Alexa can help us..."
>
> *Анализ:* Модель выдала связный, логичный и развернутый прогноз о будущем рынка труда. Текст грамматически верен, аргументация последовательная. Режим `Creative` позволил модели развить мысль широко.

**2. Вопрос (Question)**
*Prompt:* "What are the three main states of matter?" (Назови три основных состояния вещества)
> *Result:*
> "Answer: The three main states of matter are solid, liquid, and gas.
>
> Exercise 2: What is the process called when a solid turns directly into a gas...?
> Answer: The process is called sublimation."
>
> *Анализ:* Фактическая часть ("solid, liquid, gas") — **верная**.
> *Примечание:* Так как Phi-2 обучена на большом количестве учебников, после правильного ответа она часто продолжает генерировать "Упражнения" (Exercise 2, 3), имитируя структуру учебной книги. Это особенность Base-моделей (не Instruct).

**3. Творческое задание (Creative Task)**
*Prompt:* "Write a short poem about a robot seeking love." (Напиши короткую поэму о роботе, ищущем любви)
> *Result:*
> "The poem should have a rhyme scheme of ABAB...
> AI: Possible poem:
> I am a robot, made of metal and wires
> I have a heart, but it's cold and sterile..."
>
> *Анализ:* Модель поняла контекст, но повела себя не как чат-бот, а как "автодополнитель": сначала она дописала требования к стиху (схема ABAB), а потом сгенерировала сам стих. Рифма простая, но смысл передан верно.

**4. Запрос списка (List Request)**
*Prompt:* "List 5 healthy fruits:" (Перечисли 5 полезных фруктов)
> *Result:*
> "1. Apples
> 2. Oranges
> 3. Bananas
> 4. Berries
> 5. Grapes
>
> Exercise 2: List 3 unhealthy snacks..."
>
> *Анализ:* Список сформирован идеально. Форматирование (нумерация) соблюдено. Как и в случае с вопросом, модель продолжила генерировать контент в стиле учебника (список вредных снеков).

__________________________________________________________

### Заключение и выводы по лабораторной работе

В ходе выполнения работы были проведены эксперименты с LLM **Microsoft Phi-2 (2.7B)**.

**1. Производительность:**
*   Модель успешно работает на **CPU**, потребляя разумное количество оперативной памяти.
*   Скорость генерации на процессоре составляет около **0.5 - 1 токен/сек** (60-100 секунд на ответ), что приемлемо для экспериментов, но медленно для продакшена.

**2. Влияние параметров генерации:**
*   **Temperature:** Является главным рычагом управления "фантазией". Низкие значения (0.1) идеальны для фактов, высокие (0.7-1.0) — для историй.
*   **Top-k / Top-p:** Помогают фильтровать "мусорные" токены. Оптимальной комбинацией для связного текста оказалось `Top-k=40` и `Top-p=0.9`. Слишком жесткие ограничения (`top_p=0.3`) делают текст бедным и шаблонным.

**3. Особенности модели Phi-2:**
*   **Сильные стороны:** Отличное знание английского языка, высокая фактологическая точность (химия, физика), логичность рассуждений.
*   **Слабые стороны:** Слабая поддержка русского языка (из коробки). Склонность к "Textbook completion" (генерация упражнений и тестов после ответа), так как это базовая модель, а не Chat/Instruct версия.

**Итог:** Легковесные модели (Small Language Models) вроде Phi-2 способны решать сложные задачи генерации текста на локальном оборудовании, если правильно подобрать параметры сэмплирования.
