# LLM EXPERIMENTS BY SEVENTWOGTH

### В этом репозитории находятся мои эксперименты с различными LLM-моделями, которые я обучаю


__________________________________________________________

## Microsoft Phi-2 (2.7B Parameters)
### Этап 1: Базовая настройка
**Цель:** Установить окружение, загрузить модель Microsoft Phi-2 и проверить работоспособность и базовую генерацию на примере простейших промптов.

**Что я использовал:**
- Device: [NVIDIA RTX3070]
- Model: microsoft/phi-2 (2.7B)

**Тестовый запуск:**
Я реализовал функцию базовой генерации, затем дал модели следующий промпт:
__________________________________________________________
*Prompt:* "В каком году была основана Священная Римская Империя?"

*Result:*
> [Answer: The Russian Orthodox Church was established in the year 1589...]

Как видим, модель не справилась с промптом на русском языке, попробуем написать промпт на английском.
__________________________________________________________
*Prompt:* "What year was the Holy Roman Empire founded in?"

*Result:*
> [A) 962 B) 963 C) 964 D) 965 Answer: B) 963 Exercise 2:
Who was the leader of the Holy Roman Empire in 963?
A]
__________________________________________________________
*Prompt:* "324 *11 ="

*Result:*
> [$3,324.]
__________________________________________________________
**Вывод этапа 1:**
Я смог запустить модель, и реализовать генерацию простейшего текста на тривиальные промпты. Стоит заметить, что данная модель не будет давать желаемых результатов с промптами на русском.

__________________________________________________________

### Этап 2: Исследование параметра Temperature
**Цель:** Изучить влияние температуры на креативность, длину и связность генерируемого текста. Параметр `temperature` контролирует энтропию (случайность) при выборе следующего токена.

**Условия эксперимента:**
- **Prompt:** "The secret to a successful life is" (Секрет успешной жизни — это...)
- **Параметры:** `do_sample=True`, `top_k=0` (отключен для чистоты эксперимента).
- **Seed:** 42 (фиксирован для воспроизводимости).

**Результаты генерации:**

**1. Temperature = 0.1 (Максимальная определенность)**
> *Result:*
> "...to be happy and healthy."
>
> *Время:* 1.26s

**2. Temperature = 0.4 (Консервативность)**
> *Result:*
> "...to never stop learning"
>
> *Время:* 0.74s

**3. Temperature = 0.7 (Баланс)**
> *Result:*
> "...to take time for yourself," said the wise man. "No matter how busy you get, make sure to take a break and enjoy yourself. It's important to take time to relax and reflect, as this will help you stay focused on your goals and enjoy life more."
>
> *Время:* 6.26s

**4. Temperature = 1.0 (Высокая вариативность)**
> *Result:*
> "...to take time for the spiritual,” says Rebecca Thomas, author of The Maternal Mentality: A Spiritual Guide for Motherhood, and “it is not always easy when you have children. However, it is important to recognize that becoming close to God is the primary means of connecting with..."
>
> *Время:* 6.26s

__________________________________________________________
**Вывод этапа 2:**
В ходе эксперимента была выявлена четкая зависимость между температурой и характером ответа модели Phi-2:

1.  **Низкая температура (0.1 - 0.4):** Модель выбирает наиболее вероятные, "безопасные" продолжения. Ответы получаются очень короткими, банальными и грамматически простыми. Модель быстро завершает генерацию (EOS-токен выпадает почти сразу).
2.  **Средняя температура (0.7):** Наблюдается наилучший баланс. Модель сгенерировала связную мини-историю ("сказал мудрец") с логическим развитием мысли. Текст выглядит естественным и полезным.
3.  **Высокая температура (1.0):** Модель начинает "галлюцинировать". Она придумала конкретного автора ("Rebecca Thomas") и несуществующую книгу, уведя тему в специфическую область (духовность и материнство). Хотя текст грамматически верен, фактическая точность и универсальность ответа снижаются.

**Итог:** Для задач, требующих четких фактов, лучше использовать низкую температуру. Для творческого письма или развернутых ответов оптимальным диапазоном для Phi-2 является 0.6–0.8.
