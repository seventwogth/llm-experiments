### 1. Токенизатор (Tokenizer)
**Код:**
```python
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token
```

**Объяснение:**
Нейросети не умеют читать буквы или слова. Они работают только с числами (матрицами).
*   **Tokenizer** — это "словарь-переводчик". Он разбивает ваш текст на кусочки (токены). Один токен может быть словом, частью слова или даже одной буквой. Каждому токену присваивается уникальный номер (ID).
*   `AutoTokenizer.from_pretrained` — автоматически загружает нужный словарь, который использовался при обучении именно этой модели (у каждой модели свой словарь).
*   **Зачем нужен паддинг (`pad_token`)?** Нейросети обрабатывают данные пакетами (батчами) фиксированной длины. Если одно предложение короче другого, пустоту нужно заполнить "пустыми" токенами (padding). У модели Phi-2 нет специального токена для пустоты, поэтому мы говорим ей: "Используй токен конца строки (`eos_token`) как заполнитель пустоты", чтобы код не сломался.

---

### 2. Типы данных (float32 vs float16)
**Код:**
```python
dtype_frac = torch.float16 if device == "cuda" else torch.float32
```

**Объяснение:**
Это формат хранения чисел с плавающей точкой (весов модели).
*   **float32 (FP32)**: "Одинарная точность". Каждое число занимает 4 байта памяти. Это стандарт для вычислений на процессоре (**CPU**), так как он работает с ними точнее и стабильнее.
*   **float16 (FP16)**: "Половинная точность". Число занимает 2 байта.
    *   **Зачем меняем?** Если использовать `float16`, модель занимает **в 2 раза меньше видеопамяти (VRAM)** и на видеокартах (**GPU**) работает значительно быстрее.
*   **Логика кода:** Если у нас есть видеокарта (`cuda`), мы экономим память и используем `float16`. Если мы на процессоре (`cpu`), мы используем стандартный `float32`, так как `float16` на многих процессорах работает медленно или не поддерживается.

---

### 3. Инициализация модели
**Код:**
```python
model = AutoModelForCausalLM.from_pretrained(...)
```

**Объяснение:**
*   `AutoModelForCausalLM`: Класс для загрузки моделей, задача которых — "Causal Language Modeling" (предсказание следующего слова). Это архитектура GPT.
*   `trust_remote_code=True`: **Важный момент для Phi-2.** Эта модель имеет нестандартную архитектуру, код которой не встроен в старые версии библиотеки `transformers` по умолчанию. Этот флаг разрешает скачивание и выполнение скрипта модели (configuration_phi.py) напрямую из репозитория HuggingFace.
*   `device_map="auto"`: Если есть GPU, библиотека сама распределит слои модели по памяти видеокарты.

---

### 4. Процесс генерации
**Код:**
```python
inputs = tokenizer(prompt, return_tensors="pt").to(device)
with torch.no_grad():
    outputs = model.generate(...)
text = tokenizer.decode(outputs[0], skip_special_tokens=True)
```

**Объяснение:**
1.  **Токенизация:** `tokenizer(prompt)` превращает текст "Hello" в тензор (массив чисел) вида `[15496, ...]`. `.to(device)` отправляет эти числа на видеокарту или процессор.
2.  `torch.no_grad()`: **Критически важно.** При обучении нейросеть запоминает все промежуточные вычисления (градиенты), чтобы учиться. При генерации (инференсе) нам это не нужно. Отключение градиентов экономит гигабайты памяти и ускоряет работу.
3.  `model.generate()`: Запускает цикл:
    *   Взять входящие токены.
    *   Предсказать следующий токен.
    *   Добавить его к входящим.
    *   Повторить 50 раз (`max_new_tokens=50`).
4.  `decode()`: Превращает полученный массив чисел обратно в читаемый текст, удаляя служебные символы (вроде `<|endoftext|>`).

---

### 5. Температура (Temperature)
**Список:** `TEMPERATURES = [0.1, 0.4, 0.7, 1.0]`

**Объяснение:**
Температура — это коэффициент, который меняет распределение вероятностей перед выбором следующего слова.
*   **Что это?** Представьте, что модель выбирает следующее слово. У слова "кот" вероятность 60%, у слова "пёс" — 20%, у слова "стол" — 1%.
*   **Низкая t (0.1):** Разница усиливается. Вероятность "кота" станет 99%. Модель всегда будет выбирать самый очевидный вариант. Результат: скучный, но надежный и логичный текст.
*   **Высокая t (1.0):** Разница сглаживается. Шансы "кота" падают, а "пса" и "стола" растут. Модель с большей вероятностью выберет неочевидное слово. Результат: креативный, разнообразный, но может быть бред.
*   **Почему эти числа?** Это стандартный спектр от "робота" (0.1) до "творца" (1.0).

---

### 6. Функция `load_model`
**Объяснение:**
Это функция-обертка ("wrapper").
*   Она объединяет шаги 1, 2 и 3 в одном месте.
*   Ее задача — подготовить всё окружение один раз при запуске программы, чтобы не перезагружать тяжелую модель каждый раз, когда мы хотим сгенерировать новую фразу. Она возвращает готовые к работе объекты `model` и `tokenizer`.

---

### 7. Функция `experiment_temperature`
**Объяснение:**
Эта функция проводит научный эксперимент.
*   `inputs.input_ids.shape[1]`: Мы запоминаем длину *вопроса* (в токенах). Это нужно, чтобы в конце отрезать вопрос и вывести только ответ модели.
*   **Цикл `for`**: Мы прогоняем один и тот же промпт через модель несколько раз, меняя только параметр `temperature`.
*   `outputs[0][input_len:]`: Это "срез" (slicing). Мы берем весь массив токенов, который вернула модель (Вопрос + Ответ), и отрезаем начало (Вопрос). Остается только новый текст.

---

### 8. Seed (Зерно случайности)
**Код:** `set_seed(42)`

**Объяснение:**
Компьютеры не умеют генерировать по-настоящему случайные числа. Они используют сложные формулы, которые зависят от стартового числа — **Seed**.
*   **Зачем нужен?** Если мы запустим генерацию с температурой 0.7 два раза подряд *без* фиксации сида, мы получим два разных текста, потому что "случайность" выпадет по-разному.
*   **В эксперименте:** Нам нужно сравнить влияние *температуры*, а не случайности. Устанавливая `set_seed(42)` перед каждым запуском, мы заставляем модель "бросать кубики" одинаково. Если текст изменился — это точно из-за температуры, а не просто так повезло.

---

### 9. Top-k и Top-p
Это методы ограничения выборки, чтобы модель не несла полный бред.

*   **Top-k (Жесткий лимит):** "Рассматривай только K самых вероятных слов".
    *   Если K=5, модель отсортирует словарь по вероятности и выберет следующее слово только из топ-5 лидеров. Остальные 50,000 слов игнорируются, даже если они подходят по смыслу.
*   **Top-p (Nucleus Sampling, Мягкий лимит):** "Набирай слова сверху списка вероятностей, пока их сумма не достигнет P".
    *   Если P=0.9, модель берет самые вероятные слова, суммируя их проценты. Как только набралось 90%, она останавливается.
    *   *Разница:* Если модель уверена ("Мама мыла..."), в топ 90% попадет только 1 слово ("раму"). Если модель не уверена, в топ 90% попадет 50 слов. Этот метод гибче и умнее, чем Top-k.

---

### 10. Комбинации (Combinations)
**Код:** `[{"k": 50, "p": 0.9}, ...]`

**Объяснение:**
В реальных приложениях (ChatGPT, Claude) используются оба параметра одновременно.
1.  Сначала применяется **Top-k** (отсекаем хвост из совсем невероятных слов, например, тысяча самых редких).
2.  Из оставшихся применяем **Top-p** (выбираем динамическое ядро).
*   **Выбранные значения:**
    *   `k=50, p=0.9`: Классический баланс. Достаточно строго, но живо.
    *   `k=10, p=0.5`: Очень строго. Почти как низкая температура.
    *   `k=100, p=0.95`: Максимальная свобода, близкая к чистому рандому.

---

### 11. Функция `run_generation`
**Объяснение:**
Это вспомогательная ("утилитная") функция.
*   Она убирает дублирование кода. Вместо того чтобы писать 10 строчек `inputs = ...`, `model.generate...`, `decode...` в каждом цикле, мы написали их один раз здесь.
*   Она принимает все параметры (`temperature`, `top_k`, `top_p`) как аргументы, что делает ее универсальной для любого этапа эксперимента.
*   Возвращает чистый текст и время, затраченное на генерацию (для замеров производительности).

---

### 12. Блок экспериментов (Top-k / Top-p)
**Код:**
```python
# Для Top-k теста:
run_generation(..., top_k=k, top_p=1.0)

# Для Top-p теста:
run_generation(..., top_k=0, top_p=p)
```

**Объяснение (Изоляция переменных):**
Чтобы понять, как работает руль в машине, нельзя одновременно жать на тормоз. Нам нужно тестировать параметры по отдельности.
1.  **Тест Top-k:** Мы ставим `top_p = 1.0`. Значение 1.0 (100%) означает "не ограничивать выборку по вероятности". Фактически это выключает Top-p. Работает только Top-k.
2.  **Тест Top-p:** Мы ставим `top_k = 0`. В библиотеке `transformers` ноль означает "отключить лимит Top-k". Работает только Top-p.
3.  **Комбинации:** Включаем оба параметра, чтобы посмотреть, как они взаимодействуют в комплексе.
